{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1350f8c3-8583-449b-91eb-0c39862299be",
   "metadata": {},
   "source": [
    "# Part 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ff997-df01-4857-87c6-790e942f2bec",
   "metadata": {},
   "source": [
    "## Train VGG16 with pretrained weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb80f12-8c1e-4878-a604-7214c71dd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 with pretrain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "EPOCH = 30              \n",
    "BATCH_SIZE = 2048        \n",
    "\n",
    "VGG16_train_losses = []\n",
    "VGG16_train_acces = []\n",
    "VGG16_eval_losses = []\n",
    "VGG16_eval_acces = []\n",
    "\n",
    "# download CIFAR10 datasets\n",
    "train_data = datasets.CIFAR10(root='./data',\n",
    "                         train=True,                         \n",
    "                         transform=transforms.ToTensor(),    \n",
    "                         download=True\n",
    "                        )\n",
    "\n",
    "test_data = datasets.CIFAR10(root='./data',\n",
    "                        train=False,                         \n",
    "                        transform=transforms.ToTensor(),     \n",
    "                        download=True\n",
    "                        )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# VGG16 Model with pretrained weight\n",
    "model = torchvision.models.vgg16(pretrained=True) # 使用resnet18模型\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "#device : GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) \n",
    "Total_acc = 0\n",
    "\n",
    "\n",
    "# start training\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader, start=0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # forward propagation\n",
    "        outputs = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        # clear grads\n",
    "        optimizer.zero_grad()\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        num_correct = (predicted == labels).sum().item()\n",
    "        acc = num_correct / inputs.shape[0]\n",
    "        train_acc += acc\n",
    "\n",
    "    VGG16_train_losses.append(train_loss / len(train_loader))\n",
    "    VGG16_train_acces.append(train_acc / len(train_loader))\n",
    "\n",
    "    model.eval() # change to eval mode\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward\n",
    "        out = model(images)\n",
    "        loss = criterion(out, labels)\n",
    "        eval_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        num_correct = (predicted == labels).sum().item()\n",
    "        acc = num_correct / images.shape[0]\n",
    "        eval_acc += acc\n",
    "    \n",
    "    VGG16_eval_losses.append(eval_loss/len(test_loader))\n",
    "    VGG16_eval_acces.append((eval_acc/len(test_loader)))\n",
    "    if eval_acc > Total_acc:\n",
    "        torch.save(model, 'cifar10_vgg16_pretrain.pt')\n",
    "        print(\"saved best model\")\n",
    "        Total_acc = eval_acc\n",
    "    \n",
    "    print('epoch: {} \\nTrain Loss: {:.4f} \\nEval Loss: {:.4f} \\nTrain Acc: {:.4f} \\nEval Acc: {:.4f}'\n",
    "          .format(epoch, \n",
    "                  train_loss / len(train_loader), \n",
    "                  eval_loss / len(test_loader), \n",
    "                  train_acc / len(train_loader),\n",
    "                  eval_acc / len(test_loader)))\n",
    "\n",
    "array = [VGG16_train_losses,VGG16_train_acces,VGG16_eval_losses,VGG16_eval_acces]\n",
    "df = pd.DataFrame(array,index = ['train_losses', 'train_acces','eval_losses','eval_acces'])\n",
    "df.to_csv('./vgg16_pretrain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6a906-3b82-4654-937f-a3c22780d098",
   "metadata": {},
   "source": [
    "## Evaluation with test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e75527-73c8-4ab7-9166-8b36d96c2e3c",
   "metadata": {},
   "source": [
    "This part of code can be use for multiple evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129f4f-c85b-4487-866c-e52e6a529b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution with test set\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_data = datasets.CIFAR10(root='./data',\n",
    "                        train=False,                         \n",
    "                        transform=transforms.ToTensor(),     \n",
    "                        download=True\n",
    "                        )\n",
    "\n",
    "BATCH_SIZE=2048\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "model = torch.load('drive/My Drive/9417/resnet18-vgg16/distill_resnet18_resnet50_T=3b=0.5.pt')  # change this line to evaluate diffrent models\n",
    "model.eval() #change to eval mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in test_loader:\n",
    "    images, labels = data\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    out = model(images)\n",
    "    \n",
    "    _, predicted = torch.max(out.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print the testing result\n",
    "print('Accuracy:{:.4f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f8a41-306b-4c0b-a439-6fe3cf2c1545",
   "metadata": {},
   "source": [
    "## Train ResNet18 using knowledge distillation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5557ca-0cd1-478d-ae96-a50747cf626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenpuquan/anaconda3/envs/distill/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chenpuquan/anaconda3/envs/distill/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/chenpuquan/anaconda3/envs/distill/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Trying: T = 3, beta = 0.5\n",
      "saved best model\n",
      "epoch: 0 \n",
      "Train Loss: 8.9422 \n",
      "Eval Loss: 4.1816 \n",
      "Train Acc: 0.3919 \n",
      "Eval Acc: 0.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenpuquan/anaconda3/envs/distill/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/chenpuquan/anaconda3/envs/distill/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Trying: T = 3, beta = 0.7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(student_outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "soft_loss = torch.nn.KLDivLoss(reduction=\"batchmean\") \n",
    "hard_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCH = 1              \n",
    "BATCH_SIZE = 2048         \n",
    "\n",
    "temperature_list = [3, 5, 7, 10]\n",
    "beta_list = [0.5,0.7]\n",
    "\n",
    "for temp in temperature_list:\n",
    "  \n",
    "    for beta in beta_list:\n",
    "\n",
    "\n",
    "        distill_resnet18_train_losses = []\n",
    "        distill_resnet18_train_acces = []\n",
    "        distill_resnet18_eval_losses = []\n",
    "        distill_resnet18_eval_acces = []\n",
    "\n",
    "        # load the teacher model\n",
    "        teacher_model = torch.load('cifar10_vgg16_pretrain.pt') # you can access this model from https://drive.google.com/file/d/17zYxp_FfcVrkRd3UJb-uakfc4ddME0gA/view?usp=sharing\n",
    "        teacher_model.eval() # use teacher model to produce soft targets \n",
    "\n",
    "        student_model = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        train_data = datasets.CIFAR10(root='./data',\n",
    "                                train=True,                        \n",
    "                                transform=transforms.ToTensor(),    \n",
    "                                download=False\n",
    "                                )\n",
    "\n",
    "        test_data = datasets.CIFAR10(root='./data',\n",
    "                                train=False,                         \n",
    "                                transform=transforms.ToTensor(),     \n",
    "                                download=False\n",
    "                                )\n",
    "\n",
    "        # contruct dataloader\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    " \n",
    "        # optimizer\n",
    "        optimizer = optim.Adam(student_model.parameters(), lr=2e-3)\n",
    "\n",
    "        #device : GPU or CPU\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        student_model.to(device) #将模型加载到相应设备中\n",
    "\n",
    "\n",
    "        print(\"Now Trying: T = {}, beta = {}\".format(temp,beta))\n",
    "\n",
    "        Total_acc = 0.0  \n",
    "        for epoch in range(EPOCH):\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            student_model.train()\n",
    "\n",
    "            for i, data in enumerate(train_loader, start=0):\n",
    "                 \n",
    "                inputs, labels = data\n",
    "                    \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(inputs)\n",
    "\n",
    "                # using student model to produce hard targets\n",
    "                student_outputs = student_model(inputs)\n",
    "                \n",
    "                # calculate hard loss\n",
    "                student_loss = hard_loss(student_outputs, labels)\n",
    "                \n",
    "                # procduce soft loss\n",
    "                distillation_loss = F.kl_div(F.log_softmax(student_outputs / temp, dim=1), F.softmax(teacher_outputs / temp, dim=1), reduction='batchmean') * temp * temp\n",
    "                \n",
    "                # combine soft loss and hard loss \n",
    "                loss = (1 - beta) * student_loss + beta * distillation_loss\n",
    "\n",
    "                  \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # back propagation\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(student_outputs.data, 1)\n",
    "                num_correct = (predicted == labels).sum().item()\n",
    "                acc = num_correct / inputs.shape[0]\n",
    "                train_acc += acc\n",
    "\n",
    "\n",
    "        distill_resnet18_train_losses.append(train_loss / len(train_loader))\n",
    "        distill_resnet18_train_acces.append(train_acc / len(train_loader))\n",
    "\n",
    "        student_model.eval() # evaluation \n",
    "        eval_loss = 0\n",
    "        eval_acc = 0\n",
    "\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            out = student_model(images)\n",
    "            loss = hard_loss(out, labels)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            num_correct = (predicted == labels).sum().item()\n",
    "            acc = num_correct / images.shape[0]\n",
    "            eval_acc += acc\n",
    "\n",
    "\n",
    "        distill_resnet18_eval_losses.append(eval_loss/len(test_loader))\n",
    "        distill_resnet18_eval_acces.append((eval_acc/len(test_loader)))\n",
    "        \n",
    "        # save best model\n",
    "        if eval_acc > Total_acc:\n",
    "            torch.save(student_model, './distill_resnet18_fromVGG16_T={}b={}.pt'.format(temp,beta)) # you can change this line to set another save directory \n",
    "            print(\"saved best model\")\n",
    "            Total_acc = eval_acc\n",
    "\n",
    "        print('epoch: {} \\nTrain Loss: {:.4f} \\nEval Loss: {:.4f} \\nTrain Acc: {:.4f} \\nEval Acc: {:.4f}'\n",
    "                    .format(epoch, \n",
    "                    train_loss / len(train_loader), \n",
    "                    eval_loss / len(test_loader), \n",
    "                    train_acc / len(train_loader),\n",
    "                    eval_acc / len(test_loader)))\n",
    "        \n",
    "        \n",
    "        # save record to csv file\n",
    "        array = [distill_resnet18_train_losses,distill_resnet18_train_acces,distill_resnet18_eval_losses,distill_resnet18_eval_acces]\n",
    "        df = pd.DataFrame(array,index = ['train_losses', 'train_acces','eval_losses','eval_acces'])\n",
    "        df.to_csv('./distill_resnet18-VGG16_T={}beta={}.csv'.format(temp,beta)) # you can change this line to set another save directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b4e45-de06-4ba8-907a-32dfe9b38b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
